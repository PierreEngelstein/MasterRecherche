Master SystÃ¨mes Dynamiques et Signaux
MÃ©moire de master

Conception de dÃ©tecteurs
quantiques optimaux
via le calcul par intervalles

Auteur :
M. Pierre Engelstein

Encadrants :
Dr. Nicolas Delanoue
Pr. FranÃ§ois
Chapeau-Blondeau

Pr. Laurent Hardouin
Dr. Nicolas Delanoue
Pr. FranÃ§ois Chapeau-Blondeau
Jury :
Pr. SÃ©bastien Lahaye
Dr. Mehdi Lhommeau
Pr. David Rousseau

Remerciements
Je remercie Dr. Nicolas Delanoue et Pr. FranÃ§ois Chapeau-Blondeau pour
leur encadrement sur ce travail. Je remercie Ã©galement mes parents pour les
encouragements et lâ€™aide apportÃ©s sur cette annÃ©e de Master.

i

Table des matiÃ¨res

1 Introduction

1

2 Informatique quantique : Ã©lÃ©ments
2.1 Ã‰tat dâ€™un systÃ¨me quantique . . .
2.2 Mesure dâ€™un systÃ¨me quantique .
2.3 Ã‰volution dâ€™un systÃ¨me quantique

de
. .
. .
. .

base
. . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . .

3 Calcul par intervalles : Ã©lÃ©ments de base
3.1 Les intervalles . . . . . . . . . . . . . . .
3.1.1 Intervalle et boite . . . . . . . . .
3.1.2 Fonction dâ€™inclusion . . . . . . .
3.1.3 ArithmÃ©tique Ã©lÃ©mentaire . . . .
3.2 Optimisation avec les intervalles . . . . .
3.3 ImplÃ©mentation . . . . . . . . . . . . . .

3
3
4
5

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

6
6
6
7
8
9
10

4 Construction dâ€™un dÃ©tecteur quantique optimal
4.1 Formulation du problÃ¨me . . . . . . . . . . . . . .
4.2 ConvexitÃ© de lâ€™information mutuelle . . . . . . . .
4.3 Formulation des contraintes . . . . . . . . . . . .
4.4 Exemple concret . . . . . . . . . . . . . . . . . .
4.4.1 DonnÃ©es du problÃ¨me . . . . . . . . . . . .
4.4.2 RÃ©solution avec ibex . . . . . . . . . . . .
4.4.3 RÃ©solution avec notre optimiseur . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

12
12
15
17
18
18
20
20

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

5 Conclusion

23

A Dynamique des systÃ¨mes quantiques

24

B CrÃ©ation de circuits quantiques pour lâ€™encodage de fonctions
boolÃ©ennes
26

ii

C Codes dÃ©veloppÃ©s pour ibexopt
30
C.1 Optimisation avec deux Ã©tats dâ€™entrÃ©e . . . . . . . . . . . . . . 30
C.2 Optimisation avec trois Ã©tats dâ€™entrÃ©e . . . . . . . . . . . . . . 31
Bibliographie

32

iii

Table des figures

3.1
3.2
3.3
3.4

Fonction dâ€™inclusion . . . . . . . . . . . . . . . . . . . . .
Fonction dâ€™inclusion composÃ©e de moindre qualitÃ© . . . .
Optimisation naÃ¯ve . . . . . . . . . . . . . . . . . . . . .
Algorithme de maximisation par le calcul par intervalles

4.1
4.2
4.3

Information mutuelle par rapport Ã  la matrice de probabilitÃ©s 16
Interface web de visualisation de lâ€™optimisation . . . . . . . . . 21
Temps dâ€™optimisation en fonction de lâ€™angle entre Ï1 et Ï2 . . 22

B.1
B.2
B.3
B.4
B.5

Porte NOT contrÃ´lÃ©e . . . . . . . . . . . . . .
Circuit quantique pour f (x1 , x2 , x3 ) . . . . . .
Ã‰quivalent sans contrÃ´les par 0 . . . . . . . .
Circuit quantique dÃ©veloppÃ© pour f (x1 , x2 , x3 )
Simplifications successives pour f (x1 , x2 , x3 ) .

iv

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

. 7
. 8
. 9
. 11

.
.
.
.
.

27
27
28
28
29

Chapitre 1
Introduction

Le problÃ¨me de la dÃ©tection optimale est un problÃ¨me classique du traitement de lâ€™information, issu des travaux sur les signaux dans la deuxiÃ¨me
moitiÃ© du XXÃ¨me siÃ¨cle. Le problÃ¨me est de pouvoir dÃ©tecter du mieux que
lâ€™on puisse un signal contenant de lâ€™information parmi un signal plus ou moins
quelconque : Alice envoie Ã  Bob un message via un canal de transmission, et
Bob doit pouvoir retrouver optimalement le message transmis par Alice.
On sâ€™intÃ©resse au problÃ¨me Ã©quivalent dans le cadre de lâ€™information quantique : Alice envoie Ã  Bob un message codÃ© sur un support quantique, et Bob
doit pouvoir retrouver optimalement le message transmis. Alice en entrÃ©e
choisit dâ€™envoyer un signal quantique sÃ©lectionnÃ© parmi un ensemble dâ€™Ã©tats
possible avec une distribution de probabilitÃ©s spÃ©cifiÃ©e. Bob en sortie met
en Å“uvre une mesure quantique du signal reÃ§u. Il sâ€™agit alors de dÃ©terminer la mesure quantique optimale qui permet dâ€™identifier le plus efficacement
possible les Ã©tats envoyÃ©s.
Ce problÃ¨me peut se traiter en utilisant un certain nombre de critÃ¨res
de performance. Eldar, notamment, a utilisÃ© le critÃ¨re de la probabilitÃ© de
dÃ©tection correcte Ã  maximiser [1], ce qui revient Ã  un problÃ¨me de maximisation linÃ©aire sous contraintes semi-dÃ©finies positives, facile en pratique Ã 
rÃ©soudre. De mÃªme, Eldar a aussi considÃ©rÃ© le critÃ¨re de lâ€™erreur quadratique
de mesure [2], ce qui revient Ã  un problÃ¨me de minimisation quadratique sous
les mÃªmes contraintes semi-dÃ©finies positives.
Nous nous intÃ©ressons ici au critÃ¨re de lâ€™information mutuelle [3], qui
pose un problÃ¨me de maximisation de fonction non linÃ©aire sous contraintes
semi-dÃ©finies positives. Ce problÃ¨me nâ€™est en pratique pas facile Ã  rÃ©soudre.
Il peut lâ€™Ãªtre de faÃ§on approximative par exemple par des solutions de recuit.
Nous nous intÃ©ressons Ã  la rÃ©solution de ce problÃ¨me de faÃ§on garantie, en
utilisant le calcul par intervalle pour fournir un encadrement garanti, global,
de la solution optimale. Nous allons donc ainsi examiner dans ce travail, de
faÃ§on originale pour la premiÃ¨re fois Ã  notre connaissance, lâ€™application du
1

calcul par intervalles pour la conception de dÃ©tecteurs quantiques optimaux
en prÃ©sence dâ€™un critÃ¨re de performance informationnel non linÃ©aire.
Ce rapport dÃ©taille dans un premier chapitre les Ã©lÃ©ments dâ€™information
quantique relatifs au problÃ¨me traitÃ©. On dÃ©taillera par la suite les notions
de calcul par intervalle et lâ€™algorithme dâ€™optimisation utilisÃ©. En dernier lieu,
on dÃ©taillera le problÃ¨me dâ€™optimisation des dÃ©tecteurs quantiques et lâ€™application de lâ€™analyse par intervalles Ã  celui-ci.

2

Chapitre 2
Informatique quantique : Ã©lÃ©ments de base

Les notions de base dâ€™informatique quantique sont dÃ©crites dans plusieurs
ouvrages de rÃ©fÃ©rence, notamment dans [4, 5]. On prÃ©sente ici un rÃ©sumÃ© des
notions fondamentales Ã  connaÃ®tre pour la suite du rapport.
Il existe essentiellement trois principes, servant de base aux raisonnements
qui suivront. Ces principes sont confirmÃ©s jusquâ€™Ã  prÃ©sent par les expÃ©riences.

2.1

Ã‰tat dâ€™un systÃ¨me quantique

Un systÃ¨me quantique peut Ãªtre reprÃ©sentÃ© par un vecteur dâ€™Ã©tat, de la
mÃªme maniÃ¨re quâ€™un systÃ¨me physique classique. On le reprÃ©sente par la notation de Dirac, notÃ©e de la forme |Ïˆi. Ce vecteur dâ€™Ã©tat est nÃ©cessairement
de norme 1. On peut distinguer deux types dâ€™Ã©tats pour un systÃ¨me quantique : les Ã©tats de base, formant une base orthonormÃ©e dâ€™un espace vectoriel
complexe, et les Ã©tats superposÃ©s. Ces Ã©tats superposÃ©s correspondent Ã  une
combinaison linÃ©aire des Ã©tats de base. On peut Ã©crire gÃ©nÃ©ralement un Ã©tat
quantique de la faÃ§on suivante :
|Ïˆi =

X

cj |kj i .

(2.1)

j

AlgÃ©briquement, les {|kj i} forment une base orthonormÃ©e dâ€™un espace
vectoriel
de Hilbert. Les coefficients cj sont des scalaires complexes respectant
X
2
|cj | = 1.
j

Lâ€™Ã©tat dâ€™un systÃ¨me quantique peut Ãªtre gÃ©nÃ©ralisÃ© par une matrice densitÃ©, reprÃ©sentant un opÃ©rateur densitÃ© dans un espace de Hilbert de dimension N . Dans le cas dâ€™un Ã©tat pouvant Ãªtre dÃ©crit par un vecteur dâ€™Ã©tat |Ïˆi,
Ã©tat pur, lâ€™opÃ©rateur densitÃ© Ï correspondant sera donnÃ© par le produit extÃ©rieur Ï = |Ïˆi hÏˆ|. Dans le cas dâ€™un systÃ¨me incertain, bruitÃ©, on ne peut

3

pas forcÃ©ment factoriser lâ€™opÃ©rateur Ï en fonction dâ€™un |Ïˆi individuel, mais
il faudra faire intervenir une distribution probabiliste de vecteurs dâ€™Ã©tat, on
parle alors dâ€™Ã©tat melangÃ©.
Dans le cadre de lâ€™informatique quantique, on sâ€™appuie sur le systÃ¨me
quantique le plus simple, appelÃ© qubit. Ce systÃ¨me quantique est composÃ©
de deux Ã©tats de base, |0i et |1i, et des Ã©tats superposÃ©s. Similairement Ã 
lâ€™informatique classique, oÃ¹ on travaille sur le support dâ€™information le plus
Ã©lÃ©mentaire - le bit - en quantique on travaille sur le support dâ€™information
quantique Ã©lÃ©mentaire - le qubit. On dispose des mÃªmes Ã©tats de base, mais
lâ€™informatique quantique apporte les Ã©tats intermÃ©diaires superposÃ©s. Dans
la base canonique {|0i , |1i}, on note de faÃ§on gÃ©nÃ©rale lâ€™Ã©tat pur dâ€™un qubit
de la faÃ§on suivante : |Ïˆi = Î± Â· |0i + Î² Â· |1i avec |Î±|2 + |Î²|2 = 1

2.2

Mesure dâ€™un systÃ¨me quantique

Ces systÃ¨mes quantiques, physiques, doivent pouvoir Ãªtre mesurÃ©s afin
dâ€™avoir une utilitÃ©. On distingue deux types de mesures : des mesures projective, et des mesure gÃ©nÃ©ralisÃ©es (aussi appelÃ©e POVM, pour Positive OperatorValued Measure). La mesure dâ€™un systÃ¨me quantique passe par lâ€™utilisation
dâ€™opÃ©rateurs de mesure, qui sont des opÃ©rateurs linÃ©aires sur lâ€™espace des
Ã©tats.

Mesure projective
Une mesure projective est la forme la plus simple, constituÃ©e par un ensemble de N opÃ©rateurs de projection orthogonaux de rang 1 {Î }
X avec Î k =
|ki hk|. Ces opÃ©rateurs doivent vÃ©rifier la somme Ã  lâ€™identitÃ© :
Î k = Ik .
k

Lors de cette mesure, la probabilitÃ© dâ€™obtenir lâ€™Ã©tat Î k = |ki hk| en mesurant un Ã©tat Ïj est donnÃ© par :
Pr(|ki) = tr(Ïj Î k )

(2.2)

En dimension 2, on peut alors envisager la mesure dâ€™un Ã©tat !quantique
1
sur les Ã©tats de base {|0i , |1i}. La probabilitÃ© de mesurer |0i =
en ayant
0
!
Î±
|Ïˆi =
est alors donnÃ© par :
Î²

4

Pr(|0i) = tr

1 0
0 0

!

!!

|Î±|2 |Î±||Î²|
|Î±||Î²| |Î²|2

!

|Î±|2 0
= tr
0 0

Pr(|0i) = |Î±|2 ,

(2.3)
(2.4)

Mesure gÃ©nÃ©ralisÃ©e
Une mesure gÃ©nÃ©ralisÃ©e consiste similairement Ã  un ensemble de N opÃ©rateurs de mesure {Î } se sommant Ã  lâ€™identitÃ©. En revanche, ces opÃ©rateurs
de mesure, non nÃ©cessairement des projecteurs, peuvent Ãªtre Ã©tendus Ã  des
opÃ©rateurs semi-dÃ©fini positifs. De la mÃªme maniÃ¨re que les POVM sont la
gÃ©nÃ©ralisation des opÃ©rateurs de projection, ils permettent de mesurer la gÃ©nÃ©ralisation des Ã©tats quantiques qui ne sont alors pas nÃ©cessairement descriptibles par des vecteurs dâ€™Ã©tat. La diffÃ©rence avec les opÃ©rateurs de projection
est quâ€™avec un POVM, le nombre de rÃ©sultats de mesure peut diffÃ©rer de la
dimension de lâ€™espace de Hilbert des Ã©tats.
Comme pour les opÃ©rateurs de projection, la probabilitÃ© dâ€™obtenir Î k en
mesurant lâ€™Ã©tat quantique Ïj est donnÃ© par Pr(Î k ) = tr(Ïj Î k ).
Il faut noter que, lorsquâ€™on fait la mesure, on projette rÃ©ellement le systÃ¨me quantique dans lâ€™Ã©tat mesurÃ©. ConcrÃ¨tement, si on a un Ã©tat superposÃ©
quâ€™on mesure, il se place dans lâ€™Ã©tat mesurÃ© quâ€™on mesure, et si on rÃ©pÃ¨te la
mÃªme mesure on obtiendra systÃ©matiquement le mÃªme rÃ©sultat. La mesure
fait donc perdre lâ€™Ã©tat quâ€™on avait auparavant.

2.3

Ã‰volution dâ€™un systÃ¨me quantique

Entre la prÃ©paration dâ€™un systÃ¨me quantique et une mesure, on peut
souhaiter appliquer une sÃ©rie de transformations qui permettent de faire Ã©voluer le systÃ¨me et donc potentiellement effectuer des calculs. Ces possibilitÃ©s
forment la base du traitement quantique de lâ€™information et du calcul quantique (elles sont illustrÃ©es en annexe A). On peut noter Ã©galement quâ€™une
partie du premier mois de stage a Ã©tÃ© passÃ© Ã  effectuer des tests sur des
processeurs quantiques rÃ©els accessibles via internet, ceux de D-Wave notamment.

5

Chapitre 3
Calcul par intervalles : Ã©lÃ©ments de base

Le calcul par intervalles est dÃ©crit au dÃ©part dans les travaux de Ramon
Moore [6]. Lâ€™utilitÃ© de ce mode de calcul vient des problÃ¨mes que reprÃ©sente le
stockage des nombres rÃ©els dans nos ordinateurs via la norme IEEE 754. En
effet, on sait avec cette norme facilement reprÃ©senter une certaine quantitÃ©,
finie, de nombres rÃ©els tels que 0.5, sous la forme signe Ã— baseexposant Ã—
(1 + mantisse). Il est en revanche impossible de reprÃ©senter exactement la
plupart des nombres rÃ©els, tels que 0.1. De ce fait, lorsquâ€™on se place dans des
contextes de calculs, on peut se retrouver Ã  accumuler des erreurs de prÃ©cision
qui vont venir fausser les rÃ©sultats. Quand on veut garantir des rÃ©sultats, par
exemple sur des problÃ¨mes dâ€™optimisation, cela peut devenir pÃ©nalisant.
Le calcul par intervalles permet en outre de pouvoir caractÃ©riser lâ€™ensemble des solutions dâ€™un problÃ¨me, dâ€™en obtenir une caractÃ©risation globale.
Cela permet de garantir quâ€™on a bien lâ€™optimum global sur tout lâ€™ensemble
des solutions admissibles dâ€™un problÃ¨me dâ€™optimisation. Dans ce chapitre,
on prÃ©sente les notions formant la base du calcul par intervalle ainsi quâ€™un
algorithme dâ€™optimisation utilisant cette mÃ©thode de calcul.

3.1
3.1.1

Les intervalles
Intervalle et boite

DÃ©finition 1. On dÃ©finit un intervalle [x, x] comme lâ€™ensemble des nombres
rÃ©els x tels que x â‰¤ x â‰¤ x.
On note par la suite plus gÃ©nÃ©ralement [x] = [x, x].
âˆš
Exemple
1.
Si
on
veut
reprÃ©senter
le
nombre
2 = 1.4142 . . . , on peut dire :
âˆš
1.4 â‰¤ 2 â‰¤ 1.5, donc encadrer ce nombre par lâ€™intervalle [1.4, 1.5].
On Ã©tend cette notion dâ€™intervalle Ã  plusieurs variables en prenant le produit cartÃ©sien de plusieurs intervalles pour former des boites en n dimensions :
6

DÃ©finition 2. Une boite [x] est le produit cartÃ©sien des intervalles qui composent la boite : [x] = [x1 ] Ã— [x2 ] Ã— Â· Â· Â· Ã— [xn ].

3.1.2

Fonction dâ€™inclusion

Avec cette notion dâ€™intervalle, on peut dÃ©finir le comportement quand
on applique une fonction. Lâ€™idÃ©e est de se dire que, pour un intervalle ou
une boite dâ€™entrÃ©e [x], lâ€™intervalle image par une fonction f doit contenir
lâ€™ensemble des images prises par la fonction f pour tout les x âˆˆ [x] :
DÃ©finition 3. Soit f : Rn â†’ Rm une fonction, la fonction [f ] : Rn â†’ Rm
est une fonction dâ€™inclusion pour f si
âˆ€[x] âˆˆ Rn , f ([x]) âŠ‚ [f ]([x])

(3.1)

Exemple 2. La figure 3.1 montre lâ€™encadrement dâ€™une fonction y = f (x)
quelconque.

Figure 3.1 â€“ Fonction dâ€™inclusion
Les fonctions dâ€™inclusion nous intÃ©ressant ici sont celles convergentes,
câ€™est-Ã -dire celles dont la taille de la boite image tend vers 0 quand la taille
de la boite dâ€™entrÃ©e tend vers 0.
DÃ©finition 4. Une fonction dâ€™inclusion [f ] de f est dite convergente si
w([x]) â†’ 0 â‡’ w([f ]([x])) â†’ 0,

(3.2)

Avec w([x]) fonction diamÃ¨tre de la boite [x].
Lâ€™encadrement nÃ©cessite la connaissance prÃ©cise de la forme de la fonction pour pouvoir lâ€™encadrer correctement. Ceci peut se rÃ©vÃ©ler compliquÃ©
pour des fonctions non-Ã©videntes, typiquement quand on monte en dimension. Pour cela, on peut combiner les fonctions dâ€™inclusion sans perdre la
garantie dâ€™inclusion [7, 8] , comme indiquÃ© dans le ThÃ©orÃ¨me 1.
7

ThÃ©orÃ¨me 1. Si [f ] et [g] sont des fonctions dâ€™inclusion respectives pour f
et g, alors [f ] â—¦ [g] est une fonction dâ€™inclusion pour f â—¦ g.
Cela permet en pratique de construire des fonctions dâ€™inclusions Ã©lÃ©mentaires puis de les combiner. En effectuant cette opÃ©ration, on peut en revanche
perdre de la prÃ©cision sur lâ€™encadrement comme le montre la figure 3.2.

Figure 3.2 â€“ Fonction dâ€™inclusion composÃ©e de moindre qualitÃ©

3.1.3

ArithmÃ©tique Ã©lÃ©mentaire

Comme dit prÃ©cÃ©demment, on peut construire les fonctions dâ€™inclusions
des fonctions nÃ©cessaires pour nâ€™importe quel problÃ¨me en calcul par intervalle. SpÃ©cifiquement, il est utile de dÃ©finir un certain nombre de fonctions
de base permettant de former les briques de construction pour la formation
de fonctions composÃ©es. On peut ainsi dÃ©finir les opÃ©rateurs binaires (lâ€™addition, la soustraction, la multiplication, . . .) ainsi que les opÃ©rateurs unaires
(lâ€™exponentielle, la puissance, le sinus, . . .).
Exemple 3. Un certain nombre de fonctions arithmÃ©tiques Ã©lÃ©mentaires
peuvent Ãªtre formulÃ©es , avec [x1 ] = [x1 , x1 ] et [x2 ] = [x2 , x2 ] :
â€” [x1 ] + [x2 ] = [x1 + x2 , x1 + x2 ]
â€” [x1 ] âˆ’ [x2 ] = [x1 âˆ’ x2 , x1 âˆ’ x2 ]
â€” [x1 ] Ã— [x2 ] = [min(x1 x2 , x1 x2 , x1 x2 , x1 x2 ), max(x1 x2 , x1 x2 , x1 x2 , x1 x2 )]
â€” e[x] = [ex , ex ]
â€” ...
On peut Ã©tendre ces dÃ©finitions Ã  lâ€™ensemble des fonctions strictement
monotones : il est Ã©vident de se dire que, si une fonction f (x) est strictement
croissante, alors [f ]([x]) = [f (x), f (x)] est une fonction dâ€™inclusion pour f .
On peut alors construire des fonctions moins Ã©videntes, comme f : x 7â†’ x3
en dÃ©coupant la dÃ©finition de la fonction par morceaux monotones.

8

3.2

Optimisation avec les intervalles

On met en place un algorithme dâ€™optimisation utilisant le calcul par intervalle pour obtenir un encadrement garanti de la solution Ã  notre problÃ¨me.
On veut rÃ©soudre le problÃ¨me max(f (x)) tel que g(x) â‰¤ 0, avec f fonction coÃ»t et g un ensemble des contraintes. Avec le calcul par intervalles, on
cherche Ã  avoir un encadrement garanti, global de la solution au problÃ¨me. Le
principe de base est de dÃ©couper lâ€™ensemble des entrÃ©es en un certain nombre
de boites, dÃ©pendant de la prÃ©cision que lâ€™on veut, comme Ã  la figure 3.3a.
On choisit ensuite un a solution admissible du problÃ¨me suivant le thÃ©orÃ¨me
2.
ThÃ©orÃ¨me 2. Soit un a une solution admissible du problÃ¨me max f (x) tel
x
que g(x) â‰¤ 0 et xâˆ— la solution optimale, on a :
sup([f ]([x])) â‰¤ f (a) â‡’ xâˆ— âˆˆ
/ [x]

(3.3)

Cela nous permet dâ€™Ã©liminer directement de lâ€™ensemble des solutions les
boites dont la borne supÃ©rieure de lâ€™image est infÃ©rieure Ã  lâ€™image de ce candidat a, puisque garanties comme ne contenant pas lâ€™optimum du problÃ¨me.
La figure 3.3b illustre cette Ã©limination. Ã€ lâ€™issue de cette Ã©tape, on voit quâ€™on
obtient un ensemble plus restreint de boites garanties comme contenant la
solution, et on peut itÃ©rer en choisissant au fur et Ã  mesure un candidat a
meilleur, et on arrive Ã  un encadrement satisfaisant de la solution comme Ã 
la figure 3.3c avec lâ€™intervalle [x]âˆ— .

(a)

(b)

(c)

Figure 3.3 â€“ Optimisation naÃ¯ve
Cette mÃ©thode dâ€™optimisation, "naÃ¯ve", permet dâ€™obtenir un rÃ©sultat satisfaisant, mais va Ãªtre rapidement limitÃ©e si on veut des prÃ©cisions plus Ã©levÃ©es.
En effet, on va avoir trÃ¨s rapidement un trÃ¨s grand nombre de boites Ã  traiter. On peut remarquer entre autres quâ€™un certain nombre de boites vont Ãªtre
dans des "rÃ©gions" globales pouvant Ãªtre Ã©liminÃ©es (par exemple, sur la figure
9

3.3b, on a tout le bas de la fonction qui pourrait Ãªtre Ã©liminÃ© dâ€™un coup).
Cela nous amÃ¨ne Ã  un algorithme plus avancÃ©, prÃ©sentÃ© en 3.4.
Au lieu de dÃ©couper directement lâ€™espace des entrÃ©es en un trÃ¨s grand
nombre de boites, on va bisecter lâ€™espace en deux boites, suivant lâ€™axe le plus
grand. On obtient deux boites, dont lâ€™axe le plus grand aura Ã©tÃ© coupÃ© en
faisant [x] âˆ’â†’ {[x; (x + x).0.5], [(x + x).0.5; x]}. On considÃ¨re la bissection
par le milieu, mais on pourrait aussi utiliser des bissections plus avancÃ©es
permettant dâ€™accÃ©lÃ©rer lâ€™algorithme.
Une fois ces deux boites obtenues, on peut Ã©valuer la fonction dâ€™inclusion
et les contraintes, et dÃ©cider de supprimer les boites ne rentrant pas dans
les contraintes. Sur lâ€™ensemble des boites obtenues pour une itÃ©ration, on
effectue lâ€™opÃ©ration de recherche de critÃ¨re dÃ©crit prÃ©cÃ©demment, et on rÃ©itÃ¨re
sur les boites restantes. Cela permet dâ€™Ã©liminer rapidement les boites de plus
grande taille qui sont garanties ne contenant pas lâ€™optimum, et donc rÃ©duire
considÃ©rablement le nombre de boites Ã  traiter par la suite.
On considÃ¨re lâ€™algorithme fini quand on a obtenu une prÃ©cision suffisante
sur lâ€™encadrement de la fonction ou des variables dâ€™entrÃ©e, ou alors quand un
certain nombre dâ€™itÃ©rations ont Ã©tÃ© effectuÃ©es.

3.3

ImplÃ©mentation

Lâ€™algorithme 3.4 a Ã©tÃ© implÃ©mentÃ© en C# (dotnet 5) sur la base de la
librairie IntSharp modifiÃ©e pour rÃ©pondre Ã  nos besoins (rajout de lâ€™intervalle vide, de la fonction x âˆ’â†’ x log(x), des intervalles boolÃ©ens, . . .). Une
interface graphique basique utilisant Blazor a Ã©tÃ© mise en place pour faciliter la visualisation de lâ€™optimisation et des diffÃ©rents problÃ¨mes rencontrÃ©s lors du dÃ©veloppement. Lâ€™ensemble du projet est disponible sur https:
//github.com/PierreEngelstein/IntervalEval. La solution est organisÃ©e
en plusieurs modules :
â€” IntervalEval qui fourni la librairie de base pour le calcul par intervalle et lâ€™optimisation ;
â€” IntervalEval.Front est lâ€™interface web basique dÃ©veloppÃ©e avec le
framework Blazor Server ;
â€” IntervalEval.Optimizer est une interface en ligne de commande
pour le problÃ¨me spÃ©cifique dÃ©taillÃ© dans ce rapport ;
â€” IntervalEval.FrontConsole est une interface en ligne de commande
codÃ©e pour tester le problÃ¨me Ã  trois Ã©tats quantiques dâ€™entrÃ©e, pour
tester les performances sur un problÃ¨me en plus haute dimension ;
â€” IntervalEval.Tests fournit un ensemble de tests unitaires pour le
bon fonctionnement de la librairie dâ€™intervalle.
10

Data: [Iinit ] initial search box ;  stop criterion ; f cost function ; g
constraints function ;
Output: [f ] bounds of best solution ; [I] solution box
begin
solutions list of solution boxes;
Add [Iinit ] to solutions;
[fc ] current bounds of solutions;
while fc âˆ’ fc â‰¥  do
currentSolutions empty list of boxes;
/* Bisect, evaluate cost, manage constraints
*/
forall sol in solutions do
[left], [right] â†âˆ’ bisect(sol);
if g([left]) is valid then
Add [left] to currentSolutions;
end
if g([right]) is valid then
Add [right] to currentSolutions;
end
end
/* Remove boxes certified not to contain maximum */
Evaluate [f ] for all [currentSolutions];
fbest best f (sol.mid) in all [f ] ;
Remove all [sol] in [currentSolutions] where
sup([f ]([sol])) â‰¤ fbest ;
solutions â†âˆ’ currentSolutions
end
return solutions, [fc ]
end
Figure 3.4 â€“ Algorithme de maximisation par le calcul par intervalles

11

Chapitre 4
Construction dâ€™un dÃ©tecteur quantique
optimal

On prÃ©sente ici le dÃ©tail du problÃ¨me de la dÃ©tection optimale quantique
avec le critÃ¨re de lâ€™information mutuelle. Les rÃ©sultats prÃ©sentÃ©s ici ont fait
lâ€™objet dâ€™une proposition de communication Ã  la journÃ©e "Traitement du signal et applications quantiques" du GdR CNRS ISIS [9].

4.1

Formulation du problÃ¨me

Le problÃ¨me de la dÃ©tection dâ€™Ã©tat quantique porte sur un ensemble de
m Ã©tats quantiques reprÃ©sentÃ©s par les opÃ©rateurs densitÃ© {Ïj , 1 â‰¤ j â‰¤ m}
munis des probabilitÃ©s Ã  priori {pj â‰¥ 0 , 1 â‰¤ j â‰¤ m}. Lâ€™objectif est dâ€™obtenir
un ensemble de n opÃ©rateurs de mesure {Î k , 1 â‰¤ k â‰¤ n} permettant
dâ€™identifier le mieux possible selon leur probabilitÃ©s les Ã©tats dâ€™entrÃ©e qui
nous arrivent.
En dimension 2, les opÃ©rateurs Ïj et Î k sont des
! matrices Hermitiennes
a
b + ic
semi-dÃ©finies positives, de la forme
.
b âˆ’ ic
d
Plusieurs critÃ¨res ont Ã©tÃ© proposÃ©s Ã  optimiser afin de construire ces dÃ©tecteurs optimaux. Dâ€™une part, on a la possibilitÃ© de travailler sur la minimisation de lâ€™erreur quadratique de mesure [2] ou la maximisation de la probabilitÃ© de dÃ©tection correcte [1]. Dâ€™autre part, et câ€™est ce sur quoi nous avons
travaillÃ©, on peut considÃ©rer le critÃ¨re de lâ€™information mutuelle entrÃ©e-sortie
comme critÃ¨re Ã  maximiser [3]. Il sâ€™agit dâ€™un critÃ¨re prÃ©sentant une grande
pertinence pour Ã©valuer la transmission dâ€™information sur des canaux de tÃ©lÃ©communication. En particulier, lâ€™information mutuelle permet de caractÃ©riser
le dÃ©bit maximal dâ€™information quâ€™il est possible de transmettre sans erreur
sur un canal de tÃ©lÃ©communication donnÃ©.
Lâ€™information mutuelle de deux variables alÃ©atoires X et Y a Ã©tÃ© formulÃ©e
12

par Shannon en 1948 [10]. Elle est donnÃ©e en fonction des distributions de
probabilitÃ© p(X,Y ) (x, y), pX (x) et pY (y) :

I(X; Y ) =

X X

p(X,Y ) (x, y) log

p

yâˆˆY xâˆˆX

(X,Y ) (x, y)

pX (x)pY (y)



.

(4.1)

Elle peut aussi Ãªtre Ã©crite en fonction des entropies des variables alÃ©atoires :
I(X; Y ) = H(X) âˆ’ H(X|Y )
= H(Y ) âˆ’ H(Y |X)
= H(X) + H(Y ) âˆ’ H(X, Y ).

(4.2)
(4.3)
(4.4)

Avec H(X) entropie marginale de X, H(Y ) entropie marginale de Y ,
H(X|Y ) entropie conditionelle de X sachant Y et enfin H(X, Y ) entropie
conjointe de X et Y . On peut utiliser indiffÃ©rement log2 , log10 ou ln pour le
logarithme, le changement Ã©tant Ã  une constante multiplicative prÃ¨s.
Dans le cas classique, les entropies marginales, conditionnelles et conjointes
sont dÃ©finies par :
H(X) = âˆ’

X

pX (x) log(pX (x)),

(4.5)

pY (y) log(pY (y)),

(4.6)

xâˆˆX

H(Y ) = âˆ’

X
yâˆˆY

H(X, Y ) = âˆ’

X X

p(X,Y ) (x, y) log(p(X,Y ) (x, y)),

(4.7)

xâˆˆX yâˆˆY

H(Y |X) = âˆ’

X

p(X,Y ) (x, y) log

p

(X,Y ) (x, y)

xâˆˆX,yâˆˆY

pX (x)



(4.8)

Dans le cas quantique, des formules spÃ©cifiques existent pour lâ€™entropie
dâ€™un Ã©tat quantique et lâ€™information mutuelle, mais on ne les utilise pas
spÃ©cifiquement ici. En effet, on sâ€™intÃ©resse Ã  la transmission et rÃ©cupÃ©ration
dâ€™information classique, sur un canal quantique : on part dâ€™information classique, les X, quâ€™on encode dans les Ïj , pour de la transmition, et on mesure
en quantique pour rÃ©cupÃ©rer de lâ€™information classique, les Y .
On a en entrÃ©e :
P (X = Ïj ) = pj ,
13

1 â‰¤ j â‰¤ m.

(4.9)

La mesure quantique crÃ©e la distribution conditionnelle entrÃ©e sortie :
P (Î k |X = Ïj ) = tr(Ïj Î k ) = Î±jk ,

1 â‰¤ k â‰¤ n,

(4.10)

avec n 6= m possiblement. On peut en dÃ©duire la distribution de sortie :

P (Y = Î k ) =
=

m
X
j=1
m
X

P (Î k |X = Ïj )P (X = Ïj )

(4.11)

pj Î±jk .

(4.12)

j=1

Lâ€™entropie de sortie est donc dÃ©finie par :

H(Y ) = âˆ’
=âˆ’

n
X

P (Y = Î k ) log(P (Y = Î k ))

k=1
n X
m
X
k=1



pj Î±jk log

m
X

pj Î±jk



(4.13)
(4.14)

j=1

j=1

Lâ€™entropie dâ€™entrÃ©e est elle dÃ©finie par :

H(X) = âˆ’
=âˆ’

m
X
j=1
m
X

P (X = Ïj ) log(P (X = Ïj ))

(4.15)

pj log(pj )

(4.16)

j=1

La probabilitÃ© conjointe de X et de Y est donnÃ©e par :

P (X = Ïj , Y = Î k ) = pj tr(Ïj Î k ),

(4.17)

Et donc lâ€™entropie conjointe de X et de Y est donnÃ©e par :

H(X, Y ) =

m X
n
X

pj tr(Ïj Î k ).

(4.18)

j=1 k=1

On obtient donc lâ€™information mutuelle entrÃ©e sortie dÃ©pendant Ã  la fois
des pj et des Î±jk :
14

I(Ï; Î ) = H(Ï) + H(Î ) âˆ’ H(Ï, Î )
=âˆ’

m
X
j=1





pj log pj âˆ’

n X
m
X
k=1



pj Î±jk log

m
X

j=1

j=1



pj Î±jk +

m X
n
X

pj Î±jk log(pj Î±jk )

j=1 k=1

(4.19)
On peut aussi exprimer lâ€™information mutuelle en fonction de lâ€™entropie
conditionnelle, mais il est plus efficace dâ€™utiliser celle donnÃ©e Ã  lâ€™Ã©quation 4.19
pour la rÃ©solution numÃ©rique.
Finalement, le problÃ¨me se formule comme un problÃ¨me de maximization de lâ€™information mutuelle : on cherche les opÃ©rateurs de mesure Î k qui
maximisent lâ€™information mutuelle :
max I(Ï, Î )
Î 

(4.20)

tel que :
Î k  0,

1â‰¤kâ‰¤n
n
X

Î k = I

(4.21)
(4.22)

k=1

La contrainte 4.21 qui impose la semi-dÃ©finie positivitÃ© des opÃ©rateurs
de mesure Î k . Enfin, la contrainte 4.22 permet dâ€™obtenir des opÃ©rateurs de
mesure cohÃ©rents pour que les probabilitÃ©s de mesure P (Î k ) soient positives
et se somment Ã  1.
On est en prÃ©sence dâ€™une fonction non linÃ©aire, convexe, et les contraintes
engendrent un ensemble admissible convexe. Câ€™est le cas idÃ©al lors dâ€™une
minimisation, mais le problÃ¨me est une maximisation, de mÃªme difficultÃ©
quâ€™une minimisation concave, on ne peut donc pas juste faire une descente
de gradient pour le rÃ©soudre, lâ€™optimum se situe sur la frontiÃ¨re. On peut
utiliser un certain nombre de mÃ©thodes approximatives, nous utilisons le
calcul par intervalle afin dâ€™obtenir un encadrement global de la solution.

4.2

ConvexitÃ© de lâ€™information mutuelle

Davies considÃ¨re dans [3] que lâ€™information mutuelle pour ce problÃ¨me
peut Ãªtre considÃ©rÃ©e comme Ã©tant convexe, simplifiant la rÃ©solution du problÃ¨me en ayant Ã  chercher le maximum sur les bords. On sâ€™intÃ©resse ici Ã 
lâ€™Ã©tude de cette convexitÃ©.
15

Dans son article, Davies regroupe les traces et probabilitÃ©s sous une seule
variable Pjk = pj tr(Ïj Î k ). Ces coefficients Pjk forment une matrice des probabilitÃ©s, telle que :
X

Pjk = 1,

(4.23)

Pjk = pj .

(4.24)

jk

X
k

Lâ€™information mutuelle sâ€™Ã©crit donc :

I(P ) =

X
j

X

H(

k

Pjk ) +

X
k

H(

X
j

Pjk ) âˆ’

X

H(Pjk )

(4.25)

jk

La fonction H(x) = âˆ’x log(x) est convexe, et donc I est convexe par
rapport Ã  la matrice des probabilitÃ©s P . La figure 4.1 illustre cette fonction
en fixant p1 = 0.3 et p2 = 0.7.

Figure 4.1 â€“ Information mutuelle par rapport Ã  la matrice de probabilitÃ©s
La convexitÃ© semble bien vraie par rapport Ã  P , mais on cherche Ã  optimiser les matrices Î k . La matrice P comporte les traces de la multiplication
Ïj Î k , qui est linÃ©aire par rapport aux coefficients de Î k . Si la fonction I(P )
est convexe par rapport Ã  P , alors elle lâ€™est par rapport aux Î k , grace Ã  la
linÃ©aritÃ©.
Quand on trace la mÃªme fonction, mais par rapport aux variables Î Î±jk , en
se fixant dans un espace deux dimensions, on sâ€™aperÃ§oit que la fonction nâ€™est
pas correctement dÃ©finie sur les bords. Ceci est dÃ» au fait que x âˆ’â†’ x log(x)
nâ€™est pas dÃ©fini pour x < 0, ce qui fausse ou bloque les calculs, suivant
lâ€™implÃ©mentation.
16

4.3

Formulation des contraintes

La dÃ©finition du problÃ¨me permet de rÃ©soudre notamment les cas immÃ©diats des opÃ©rateurs densitÃ© Ïj orthogonaux, mais la rÃ©solution devient
trÃ¨s lente lorsquâ€™on passe Ã  dâ€™autres cas non orthogonaux. On rajoute des
conditions au problÃ¨me pour accÃ©lÃ©rer la rÃ©solution.
PremiÃ¨rement, on sait que les opÃ©rateurs de mesure se somment Ã  lâ€™identitÃ©. Cela signifie quâ€™on peut passer dâ€™un problÃ¨me Ã  n matrices Ã  un problÃ¨me
Ã  n âˆ’ 1 matrices pour n â‰¥ 2. Les matrices Ã©tant carrÃ©es de dimension N , on
passe de n Ã— N 2 variables Ã  (n âˆ’ 1) Ã— N 2 variables, ce qui est non nÃ©gligeable.
De plus, le problÃ¨me et les contraintes sont symÃ©triques, une permutation
des Î k nâ€™influence pas le rÃ©sultat de la fonction coÃ»t. Cela nous permet de
couper le problÃ¨me au moins en deux pour rÃ©duire Ã  nouveau le temps de
calcul. Du fait de la somme Ã  lâ€™identitÃ©, on peut ajouter en contrainte que
1
, etc.
Î 11,1 â‰¤ n1 pour n opÃ©rateurs de mesure, puis Î 21,1 â‰¤ nâˆ’1
Ensuite, on peut exprimer la semi-dÃ©finie positivitÃ© des opÃ©rateurs de
mesure en utilisant le critÃ¨re de Sylvester. Dans le cas gÃ©nÃ©ral, il indique que
le dÃ©terminant de la matrice doit Ãªtre positif ou nul, ainsi que les n mineurs
principaux. Dans le cas Ã  deux dimension, cela se traduit par le dÃ©terminant
et les deux Ã©lÃ©ments diagonaux postitifs ou nuls. On a ainsi des contraintes
quadratiques sur les entrÃ©es.
Enfin, pour rappel, les opÃ©rateurs de mesure sont des opÃ©rateurs qui ne
sont pas nÃ©cessairement des projecteurs de rang 1. Pour quâ€™ils soient de rang
1, il faudrait entre autres que tr(Î k ) = 1. On peut considÃ©rer quâ€™on restreint
le problÃ¨me Ã  un cas de rang 1, et dans ce cas rajouter la contrainte que
la somme des Ã©lÃ©ments diagonaux des opÃ©rateurs de mesure doit sommer
Ã  1. Cela permet soit de retirer une variable par opÃ©rateur de mesure au
problÃ¨me, en lâ€™exprimant par xn+1 = 1âˆ’

n
X

xi avec les xi Ã©lÃ©ments diagonaux

i=1

de lâ€™opÃ©rateur de mesure, ce qui nÃ©cessite une reformulation du problÃ¨me, soit
lâ€™ajout de la contrainte.

17

4.4

Exemple concret

4.4.1

DonnÃ©es du problÃ¨me

ConsidÃ©rons deux Ã©tats purs quantiques :

|Ïˆ1 i =

ï£« 1 ï£¶
3
ï£¬
ï£·
ï£­ âˆš ï£¸,
2 2
3

ï£¶

ï£«

âˆš1
ï£¬ 2ï£·

|Ïˆ2 i = ï£¬
ï£­

âˆš1
2

(4.26)

ï£·,
ï£¸

avec les probabilitÃ©s prÃ©alables :
p1 = P (|Ïˆ1 i) = 0.1,
p2 = P (|Ïˆ2 i) = 0.9.

(4.27)
(4.28)

Ces deux Ã©tats peuvent Ãªtre rÃ©Ã©crits sous la forme dâ€™opÃ©rateurs densitÃ© :
ï£«
ï£¬

Ï1 = ï£¬
ï£­

âˆš ï£¶
2 2
9 ï£·

1
9
âˆš
2 2
9

8
9

ï£·,
ï£¸

ï£«1
ï£¬2

Ï2 = ï£­

1
2

1
2
1
2

ï£¶

(4.29)

ï£·
ï£¸

On a aussi deux opÃ©rateurs de mesure inconnus en dimension 2, soit 8
variables inconnues :
a1
Î 1 =
b1 âˆ’ ic1

!

b1 + ic1
,
d1

a2
Î 2 =
b2 âˆ’ ic2

b2 + ic2
d2

!

(4.30)

On peut dÃ©jÃ  calculer lâ€™information mutuelle maximum qui est H(X) [11]
puisquâ€™on a la distribution de probabilitÃ©s de X :
cmax = âˆ’

X

p(Xj ) log2 (p(Xj )) = âˆ’0.1 log2 (0.1) âˆ’ 0.9 log2 (0.9) = 0.469 Shannon,

j

(4.31)
Quelque soient les mesures Î k choisies, on ne pourra pas obtenir une
information mutuelle supÃ©rieure Ã  0.469 qui est lâ€™information maximale en
entrÃ©e.
On veut rÃ©soudre le problÃ¨me de la maximisation de lâ€™information mutuelle en fonction des {Ï} fixÃ©s et des {Î } Ã  ajuster :

18

max I(Ï1 , Ï2 , Î 1 , Î 2 )

(4.32)

Î 1 ,Î 2



â‡” max âˆ’ (Î±11 + Î±12 ) log(Î±11 + Î±12 ) âˆ’ (Î±21 + Î±22 ) log(Î±21 + Î±22 )
Î 1 ,Î 2

âˆ’ (Î±11 + Î±21 ) log(Î±11 + Î±21 ) âˆ’ (Î±12 + Î±22 ) log(Î±12 + Î±22 )


+ (Î±11 ) log(Î±11 ) + (Î±12 ) log(Î±12 ) + (Î±21 ) log(Î±21 ) + (Î±22 ) log(Î±22 ) ,
avecÎ±jk = pj tr(Ïj Î k )
tel que :
Î 1  0, Î 2  0
Î 1 + Î 2 = I

(4.33)
(4.34)

On peut rÃ©duire considÃ©rablement le nombre de variables. En effet, lâ€™Ã©quation 4.22 nous indique que les {Î } se somment Ã  lâ€™identitÃ©, on peut donc rÃ©Ã©crire Î 2 en fonction uniquement de Î 1 pour rÃ©duire Ã  4 variables {a1 , b1 , c1 , d1 } :
1 âˆ’ a1
Î 2 = I2 âˆ’ Î 1 =
âˆ’b1 + ic1

âˆ’b1 âˆ’ ic1
1 âˆ’ d1

!

(4.35)

Enfin, on voit que dans ce cas particulier les Ã©tats quantiques dâ€™entrÃ©e sont
purs et ne comportent aucun terme complexe, ce qui permet immÃ©diatement
de retirer le terme c du systÃ¨me puisquâ€™il ne sera jamais pris en compte. On
se retrouve donc au final avec 3 variables {a1 , b1 , d1 }.
On pose ensuite les contraintes sur ces variables. Tout dâ€™abord, ces variables sont dÃ©finies sur ces bornes spÃ©cifiques : a1 âˆˆ [0, 0.5], b1 âˆˆ [âˆ’1, 1],
d1 âˆˆ [0, 1]. La dÃ©termination de la semi-dÃ©finie positivitÃ© passe par les diagonales et le dÃ©terminant strictement positifs, dâ€™une part avec les bornes prÃ©cÃ©dentes et dâ€™autre part avec deux nouvelles contraintes sur les 3 variables.
Le problÃ¨me sâ€™Ã©crit donc :
max I(a1 , b1 , d1 ),

a1 ,b1 ,d1

tel que :
a1 âˆˆ [0, 0.5], b1 âˆˆ [âˆ’1, 1], d1 âˆˆ [0, 1],
a1 Ã— d1 âˆ’ b21 â‰¥ 0,
(1 âˆ’ a1 ) Ã— (1 âˆ’ d1 ) âˆ’ b21 â‰¥ 0.
19

4.4.2

RÃ©solution avec ibex

On peut tout dâ€™abord rÃ©soudre le problÃ¨me avec ibexopt. Câ€™est un optimiseur garanti sous contraintes, faisant partie de la librairie de calcul par
intervalles ibex. Les codes pour la rÃ©solution de ce problÃ¨me sont disponibles
en annexe C. On obtient une information mutuelle de 0.0681 Shannon, avec
une prÃ©cision relative de 1 Ã— 10âˆ’3 . On obtient les opÃ©rateurs de mesure suivant :
!
!
0.454 âˆ’0.498
0.546 0.498
Î 1 =
, Î 2 =
,
âˆ’0.498 0.546
0.498 0.454
avec un temps de calcul de 87 secondes.
Pour la rÃ©solution avec ibex, plusieurs modifications et ajouts doivent Ãªtre
faits. Tout dâ€™abord, le langage utilisÃ© par lâ€™outil dâ€™optimisation, minibex, ne
considÃ¨re que les problÃ¨mes de minimisation. Cela nÃ©cessite simplement de
prendre la fonction opposÃ©e puisque max(f (x)) â‡” min(âˆ’f (x)). On doit ensuite prendre lâ€™opposÃ© de lâ€™intervalle solution pour obtenir le rÃ©sultat final.
Ensuite, le calcul par intervalle fait apparaÃ®tre une difficultÃ© quant Ã  lâ€™information mutuelle. On se retrouve Ã  avoir de faÃ§on rÃ©pÃ©tÃ©e des termes du type
x Ã— log(x). Cette fonction est clairement dÃ©finie sur [0, +âˆ[ mais ne lâ€™est
pas sur ] âˆ’ âˆ, 0[. De part le pessimisme du calcul par intervalle, on peut
se retrouver Ã  avoir des intervalles complÃ¨tement nÃ©gatifs ou contenant 0 en
entrÃ©e de la fonction. On se retrouve donc avec un comportement mal dÃ©fini
dans certains cas. La solution pour ibex est de rÃ©implÃ©menter lâ€™opÃ©rateur
xlog en considÃ©rant quâ€™en dehors ou sur le bord des bornes de dÃ©finition on
considÃ¨re que la fonction renvoie 0.
Un autre point Ã  noter est la grande variabilitÃ© des temps de calculs
suivant les problÃ¨mes qui sont traitÃ©s.

4.4.3

RÃ©solution avec notre optimiseur

On a rencontrÃ© plusieurs problÃ¨mes avec ibex qui nous ont poussÃ© Ã  dÃ©velopper notre propre optimiseur.
Tout dâ€™abord, en Ã©tudiant la fonction information mutuelle, on voit que
cette fonction est convexe. Cela permet de considÃ©rablement rÃ©duire lâ€™espace
de dÃ©finition en cherchant le maximum sur le bord de la fonction au lieu de
chercher sur toute la fonction. Ceci nâ€™est pas implÃ©mentÃ© par dÃ©faut sur ibex.
Ensuite, sur lâ€™implÃ©mentation de lâ€™algorithme dâ€™optimisation, on peut voir
quâ€™ibexopt nâ€™utilise quâ€™un seul thread pour son travail. On lâ€™a vu en figure
3.4, la premiÃ¨re boucle est facilement parallÃ©lisable, ce qui permet dâ€™accÃ©lÃ©rer
considÃ©rablement le temps de rÃ©solution, et ce en fonction du matÃ©riel Ã 
disposition.
20

Enfin, ibexopt ne permet pas lâ€™accÃ¨s aux boites en place au cours de
lâ€™Ã©volution de lâ€™algorithme. Cela ne permet pas une visualisation plus dÃ©taillÃ©e de ce qui est effectuÃ©, quelles rÃ©gions sont enlevÃ©es au fur et Ã  mesure.
Dans le cas de problÃ¨mes "simples", ce nâ€™est pas forcÃ©ment important, en revanche avec un problÃ¨me comme lâ€™information mutuelle, oÃ¹ le comportement
en fonction des variables dâ€™entrÃ©e nâ€™est pas immÃ©diatement visible, câ€™est utile
pour pouvoir avoir une idÃ©e des influences des contraintes. Un exemple de
cette visualisation est disponible en figure 4.2. Les coordonnÃ©es (x, y, z) correspondent aux variables (a1 , b1 , d1 ), et les boites sont colorÃ©es en fonction
des contraintes satisfaites.
Notre optimiseur nous donne les mÃªmes solutions quâ€™ibex, Ã  la diffÃ©rence
quâ€™on arrive Ã  une plus grande prÃ©cision bien plus rapidement, en 13 secondes
pour les mÃªmes donnÃ©es.
On peut noter entre autres, Ã  la fois sur ibexopt et sur notre optimiseur
que les temps de calculs dÃ©pendent du problÃ¨me quâ€™on a. La figure 4.3 montre
par exemple que, plus lâ€™angle entre les deux Ã©tats sâ€™approche de 90 degrÃ©s et
des 180 degrÃ©s, qui sont des cas immÃ©diats de problÃ¨mes orthogonaux, plus
les temps de rÃ©solution sont bien moindres.

Figure 4.2 â€“ Interface web de visualisation de lâ€™optimisation
21

Figure 4.3 â€“ Temps dâ€™optimisation en fonction de lâ€™angle entre Ï1 et Ï2

22

Chapitre 5
Conclusion

On a ainsi pu voir comment rÃ©pondre au problÃ¨me de la dÃ©tection optimale quantique en utilisant un critÃ¨re diffÃ©rent de ceux utilisÃ©s, lâ€™information
mutuelle. Une approche par intervalle, nouvelle pour cette question, a Ã©tÃ© appliquÃ©e pour obtenir un encadrement garanti dâ€™une part du maximum atteint
par le critÃ¨re et dâ€™autre part de la mesure optimale.
Deux implÃ©mentations sont proposÃ©es, la premiÃ¨re en utilisant un optimiseur existant, ibexopt, nÃ©cessitant certaines modifications pour sâ€™adapter
au problÃ¨me posÃ©. Un deuxiÃ¨me est proposÃ©, en C#, amÃ©liorant entre autres
la vitesse de rÃ©solution par rapport Ã  ibexopt.
Les implÃ©mentations proposÃ©es ne traitent que du cas avec deux Ã©tats
dâ€™entrÃ©e et deux opÃ©rateurs de mesure, le tout sans termes complexe, et la
rÃ©solution nâ€™est dÃ©jÃ  pas immÃ©diate. En testant sur dÃ©jÃ  trois Ã©tats dâ€™entrÃ©e
et trois opÃ©rateurs de mesure (code ibexopt en annexe C.2), les temps de
rÃ©solution deviennent bien trop important. Il y a donc du travail Ã  faire
de ce cÃ´tÃ© si on veut augmenter en nombre dâ€™Ã©tats Ã  dÃ©tecter ou nombre
dâ€™opÃ©rateurs de mesure Ã  construire.
Ce problÃ¨me de dÃ©tection optimale nâ€™a en revanche Ã©tÃ© traitÃ© que dans
son cas le plus simple, en considÃ©rant un systÃ¨me "parfait" oÃ¹ aucun bruit
nâ€™apparaÃ®t. Il serait intÃ©ressant de voir plus loin en considÃ©rant les Ã©tats
dâ€™entrÃ©e bruitÃ©s. Le problÃ¨me serait alors de trouver la meilleure configuration
Ã©tats dâ€™entrÃ©e - opÃ©rateurs de mesure pour que, en prÃ©sence de bruit, on
obtienne une communication optimale.

23

Annexe A
Dynamique des systÃ¨mes quantiques

Comme nâ€™importe quel systÃ¨me physique, on peut faire Ã©voluer un systÃ¨me quantique dans le temps. Lâ€™Ã©volution dâ€™un systÃ¨me quantique est effectuÃ©e via une Ã©volution linÃ©aire de son vecteur dâ€™Ã©tat. Cette Ã©volution linÃ©aire
est reprÃ©sentÃ©e par un opÃ©rateur linÃ©aire sur H, donc par une matrice a partir
du moment oÃ¹ une base de rÃ©fÃ©rence a Ã©tÃ© choisie. Cette Ã©volution linÃ©aire doit
Ã©galement rester en accord avec le premier principe 2.1, câ€™est-Ã -dire conserver
la norme unitÃ© du vecteur dâ€™Ã©tat. La matrice dâ€™Ã©volution doit donc Ã©galement
Ãªtre unitaire.
En pratique, ces Ã©volutions de systÃ¨mes quantiques peuvent Ãªtre rÃ©alisÃ©es
par des portes logiques de faÃ§on similaire Ã  la logique boolÃ©enne classique.
Ces portes quantiques sont complÃ¨tement caractÃ©risÃ©es par la faÃ§on dont elles
transforment les Ã©tats quantiques dans la base canonique. On peut alors utiliser des tables de vÃ©ritÃ© pour les dÃ©finir, de la mÃªme faÃ§on quâ€™en informatique
classique :
1. La porte de Hadamard H. Elle permet de passer un qubit dâ€™un Ã©tat
de base |0i Ã  lâ€™Ã©tat superposÃ© âˆš12 |0i + âˆš12 |1i, ou de lâ€™Ã©tat de base |1i Ã 
lâ€™Ã©tat superposÃ© âˆš12 |0iâˆ’ âˆš12 |1i. Elle est trÃ¨s utilisÃ©e en dÃ©but de circuit
pour prÃ©parer les qubits entrants dans un Ã©tat permettant lâ€™Ã©valuation
parallÃ¨le de toutes les entrÃ©es ;
2. Les portes de Pauli X, Y et Z permettant dâ€™effectuer des rotations
aux Ã©tats des qubits ;
3. La porte de Toffoli, similaire dâ€™un NON boolÃ©en Ã  3 qubit (il effectue
un NON sur le dernier qubit quand les deux premiers sont Ã  |1i), est
une porte universelle quantique [12]. Elle permet donc de construire
lâ€™ensemble des autres portes faisables.
Avec ces portes, on vient construire des circuits quantiques permettant
de rÃ©aliser des algorithmes. Lâ€™annexe B montre un exemple de technique de
rÃ©alisation de circuits. En algorithmes majeurs, on peut citer :
24

1. Lâ€™algorithme de Deutsch-Jozsa [13] qui permet de rÃ©soudre en une
opÃ©ration le problÃ¨me de diffÃ©rentiation entre une fonction boolÃ©enne
constante et une fonction boolÃ©enne Ã©quilibrÃ©e. Il faut classiquement
2n âˆ’ 1 opÃ©rations pour rÃ©soudre ce problÃ¨me.
âˆš
2. Lâ€™algorithme de Grover [14] qui permet de rÃ©soudre en O( N ) opÃ©rations le problÃ¨me de recherche dans une liste non triÃ©e. Il faut classiquement au pire N opÃ©rations pour effectuer une recherche dans une
liste non triÃ©e.
3. Lâ€™algorithme de Shor [15] qui permet de rÃ©soudre le problÃ¨me de factorisation en nombres premiers. Câ€™est un problÃ¨me classiquement trÃ¨s
difficile Ã  rÃ©soudre, de complexitÃ© exponentielle.
Ces trois algorithmes montrent les gains de performance que permet dâ€™obtenir le calcul quantique, qui sont inacessibles avec les technologies dâ€™informatique classique.

25

Annexe B
CrÃ©ation de circuits quantiques pour lâ€™encodage de fonctions boolÃ©ennes

On Ã©tudie ici la problÃ©matique de pouvoir construire systÃ©matiquement
une fonction boolÃ©enne avec un ordinateur quantique, prÃ©sentÃ© par Younes
et Miller en 2003 [16].
En informatique classique, lâ€™ensemble des fonctions boolÃ©ennes peuvent
Ãªtre dÃ©crites Ã  lâ€™aide des opÃ©rateurs NAND et NOR. Il sâ€™agit donc de pouvoir les transcrire en quantique, et de pouvoir Ã©tablir un systÃ¨me de combinaison de ces portes, pour permettre lâ€™Ã©laboration des circuits.
Dans ce modÃ¨le, on considÃ¨re un registre de n qubits composant lâ€™entrÃ©e
du systÃ¨me, un registre de m qubits composant la sortie du systÃ¨me, et un
registre de k qubits auxiliaires pour certaines opÃ©rations intermÃ©diaires.
Pour cette construction, on se base sur la porte quantique X et ses Ã©quivalents composÃ©s CNOT, CCNOT (Toffoli), etc. On fournit alors un certain
nombre de circuits de base pouvant Ãªtre recomposÃ©s pour former des circuits
plus compliquÃ©s.
La compilation dâ€™une fonction boolÃ©enne passe alors par 4 Ã©tapes :
1. Ã‰criture de la table de vÃ©ritÃ©,
2. Pour chaque sortie donnant 1, former une porte NOT controlÃ©e.
Chaque entrÃ©e va servir de contrÃ´le, par 1 si lâ€™entrÃ©e est Ã  1, et par 0
si lâ€™entrÃ©e est Ã  0,
3. DÃ©velopper le circuit rÃ©sultant pour nâ€™avoir que des portes NOT
controlÃ©es par 0,
4. Simplifier le circuit en Ã©liminant les doublons.

Ã‰tape 1 : Ã©tablissement des premiÃ¨res portes contrÃ´lÃ©es
La figure B.1 reprÃ©sente une porte NOT contrÃ´lÃ©e. On note que les qubits
de contrÃ´le sont indiquÃ©s par â€¢ (contrÃ´le par 1) et par â—¦ (contrÃ´le par 0). Le
26

dernier qubit est la cible (target). On effectue lâ€™opÃ©ration NOT sur la cible
si et seulement si les bits de contrÃ´les respectent leur condition (si il sont Ã  1
pour ceux qui sont contrÃ´lÃ©s par 1, et si ils sont Ã  0 pour ceux contrÃ´lÃ©s par
0).
|x0 i
|x1 i
|x2 i
|x3 i

â€¢

Figure B.1 â€“ Porte NOT contrÃ´lÃ©e
Cette porte en revanche ne peut pas Ãªtre construite, on ne dispose en effet
que des portes NOT contrÃ´lÃ©es par 1 et pas de celles contrÃ´lÃ©es par 0.
Exemple 4. Soit la fonction boolÃ©enne f (x1 , x2 , x3 ) = (x1 âˆ§x2 )âˆ¨(x3 âˆ§Â¬x2 )âˆ¨
(x1 âˆ§ x3 ). Sa table de vÃ©ritÃ© est la suivante :
x1 x2 x3 F (x1 , x2 , x3 )
0 0 0
0
0 0 1
1
0 1 0
0
0 1 1
0
1 0 0
0
1 0 1
1
1 1 0
1
1 1 1
1
On a quatre sorties Ã  1. La figure B.2 reprÃ©sente donc le circuit initial
quâ€™on obtient.
|x1 i
|x2 i
|x3 i
|xf i

â€¢
â€¢

â€¢
â€¢

â€¢

â€¢
â€¢
â€¢

Figure B.2 â€“ Circuit quantique pour f (x1 , x2 , x3 )

Ã‰tape 2 : DÃ©veloppement du circuit
La deuxiÃ¨me Ã©tape consiste Ã  prendre le circuit obtenu prÃ©cÃ©demment et
Ã  le dÃ©velopper de faÃ§on Ã  nâ€™obtenir que des portes NOT contrÃ´lÃ©es par 0.
Dans le principe, une porte ayant un mÃ©lange de contrÃ´le par 0 et par 1 va
Ãªtre Ã©quivalent Ã  la combinaison des portes contrÃ´lÃ©es par 0, qui vont avoir
27

des contrÃ´les de moins en combinaison sur les contrÃ´les par 1. Un exemple
est plus clair pour comprendre. Reprenons la porte de la figure B.1. Elle est
en fait Ã©quivalente au circuit B.3 :
|x0 i
|x1 i
|x2 i
|x3 i

â€¢
â‰¡

â€¢
â€¢
â€¢

|x0 i
|x1 i
|x2 i
|x3 i

â€¢
â€¢

â€¢
â€¢

â€¢

Figure B.3 â€“ Ã‰quivalent sans contrÃ´les par 0
Exemple 5. On reprends notre exemple de fonction boolÃ©enne f (x1 , x2 , x3 ) =
(x1 âˆ§ x2 ) âˆ¨ (x3 âˆ§ Â¬x2 ) âˆ¨ (x1 âˆ§ x3 ). Une fois dÃ©veloppÃ©e, son circuit Ã©quivalent
est illustrÃ© Ã  la figure B.4
|x0 i
|x1 i
|x2 i
|xf i

â€¢
â€¢
â€¢

â€¢
â€¢

â€¢
â€¢

â€¢

â€¢
â€¢
â€¢

â€¢
â€¢

â€¢
â€¢
â€¢

â€¢
â€¢

â€¢
â€¢
â€¢

Figure B.4 â€“ Circuit quantique dÃ©veloppÃ© pour f (x1 , x2 , x3 )

Ã‰tape 3 : Simplification du circuit
La derniÃ¨re Ã©tape permet dâ€™obtenir un circuit comportant le moins de
portes possibles, en se basant sur le principe suivant : lorsquâ€™un circuit (ici
composÃ© de CNOT) est entourÃ© de deux mÃªmes CNOT, alors celles-ci sâ€™annulent et on peut alors enlever la paire doublon sans changer le rÃ©sultat.
En effectuant ce raisonnement rÃ©cursivement, on arrive Ã  obtenir un circuit
minimal.
En reprenant lâ€™exemple de la fonction boolÃ©enne prÃ©cÃ©dente, on peut faire
par Ã©tapes la simplification :

28

|x0 i
|x1 i
|x2 i
|xf i

â€¢
â€¢

â€¢
â€¢

â€¢

â€¢
â€¢
â€¢

â€¢
â€¢

â€¢
â€¢
â€¢

â€¢
â€¢

|x0 i
|x1 i
|x2 i
|xf i

(a) PremiÃ¨re simplification

|x0 i
|x1 i
|x2 i
|xf i

â€¢
â€¢

â€¢

â€¢
â€¢
â€¢

â€¢
â€¢
â€¢

â€¢
â€¢

(b) DeuxiÃ¨me simplification

â€¢
â€¢

â€¢

â€¢
â€¢

(c) DerniÃ¨re simplification

Figure B.5 â€“ Simplifications successives pour f (x1 , x2 , x3 )

Extension Ã  des circuits plus compliquÃ©s
Avec cette mÃ©thode, il est facile de construire lâ€™ensemble des fonctions
boolÃ©ennes Ã  2 bits, ainsi quâ€™a 3 bits. Pour construire des circuits plus complexes, on dispose alors de deux faÃ§ons de procÃ©der.
Tout dâ€™abord, on peut refaire ces Ã©tapes sur la fonction boolÃ©enne plus
complexe, et trouver un circuit minimal Ã©tant composÃ© de n entrÃ©es, et dâ€™un
seul qubit auxiliaire pour la sortie.
En revanche, si on ne veut pas Ã©tablir la table de vÃ©ritÃ© avant la construction du circuit, on peut utiliser les fonctions Ã©lÃ©mentaires et construire un
circuit a partir de la chaine de caractÃ¨re reprÃ©sentant la fonction. Il est important de noter que la mÃ©thode prÃ©sentÃ©e ici ne modifie pas les entrÃ©es de la
fonction, on peut donc les rÃ©utiliser comme on le souhaite pour venir greffer
des fonctions supplÃ©mentaires au circuit. Cette mÃ©thode sâ€™apparente algorithmiquement Ã  la compilation des fonctions sur les ordinateurs classiques.
En revanche, par rapport au circuit minimal quâ€™on pourrait trouver, on aura
ici un qubit auxiliaire par sous-circuit, et un nombre de portes bien plus important. Pour des considÃ©rations de limitations matÃ©rielles qui sont pour le
moment importantes, cette deuxiÃ¨me faÃ§on de procÃ©der peut ne pas donner
des rÃ©sultats implÃ©mentables pour de trop grosses fonctions.

29

Annexe C
Codes dÃ©veloppÃ©s pour ibexopt

C.1
1
2
3
4
5
6
7
8
9
10
11
12

Optimisation avec deux Ã©tats dâ€™entrÃ©e

Constants
m_size = 2 ;
P1 [ m_size ] [ m_size ] = ( ( 0 . 0 1 1 1 1 1 1 1 1 1 , 0 . 0 3 1 4 2 6 9 6 8 1 ) ;
P2 [ m_size ] [ m_size ] = ( ( 0 . 4 5 , 0 . 4 5 ) ; ( 0 , 0 . 4 5 ) ) ;

(0 ,

0.0888888889) ) ;

// M: measurement o p e r a t o r
// P : d e n s i t y o p e r a t o r
f u n c t i o n t r (M[ m_size ] [ m_size ] , P [ m_size ] [ m_size ] )
r e t u r n P ( 1 ) ( 1 ) âˆ— M( 1 ) ( 1 ) + 2âˆ—M( 1 ) ( 2 ) âˆ—P ( 1 ) ( 2 ) + 2âˆ—M( 2 ) ( 1 ) âˆ—P ( 2 ) ( 1 ) + M( 2 ) ( 2 ) âˆ—P ( 2 ) ( 2 ) ;
end
f u n c t i o n EntropyM (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _P1 [ m_size ] [ m_size ] , _P2 [
m_size ] [ m_size ] )
sum_m1 = t r (_M1, _P1) + t r (_M1, _P2) ;
sum_m2 = t r (_M2, _P1) + t r (_M2, _P2) ;
r e t u r n âˆ’x l o g (sum_m1)âˆ’x l o g (sum_m2) ;
end

13
14
15
16
17
18 f u n c t i o n EntropyP (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _P1 [ m_size ] [ m_size ] , _P2 [
m_size ] [ m_size ] )
19
sum_p1 = t r (_M1, _P1) + t r (_M2, _P1) ;
20
sum_p2 = t r (_M1, _P2) + t r (_M2, _P2) ;
21
r e t u r n âˆ’x l o g ( sum_p1 )âˆ’x l o g ( sum_p2 ) ;
22 end
23
24 f u n c t i o n EntropyMP (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _P1 [ m_size ] [ m_size ] , _P2 [
m_size ] [ m_size ] )
25
r e t u r n âˆ’x l o g ( t r (_M1, _P1) )âˆ’x l o g ( t r (_M1, _P2) )âˆ’x l o g ( t r (_M2, _P1) )âˆ’x l o g ( t r (_M2, _P2) ) ;
26 end
27
28 f u n c t i o n M u t u a l I n f o r m a t i o n (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _P1 [ m_size ] [ m_size
] , _P2 [ m_size ] [ m_size ] )
29
r e t u r n EntropyM (_M1, _M2, _P1 , _P2) + ( âˆ’0.10 âˆ— l n ( 0 . 1 0 ) âˆ’0.90 âˆ— l n ( 0 . 9 0 ) ) âˆ’
EntropyMP (_M1, _M2, _P1 , _P2) ;
30 end
31
32 f u n c t i o n I (_P1 [ m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _m1a , _m1b , _m1c , _m1d)
33
_M1 = ( ( _m1a , _m1b) ; ( _m1c , _m1d) ) ;
34
_M2 = ((1 âˆ’_m1a , âˆ’_m1b) ; (âˆ’_m1c , 1âˆ’_m1d) ) ;
35
r e t u r n M u t u a l I n f o r m a t i o n (_M1, _M2, _P1 , _P2) ;
36 end
37
38 V a r i a b l e s
39
M1_a i n [ 0 , 0 . 5 ] ,
M1_b i n [ âˆ’1 , 1 ] , M1_d i n [ 0 , 1 ] ;
40
41 M i n i m i z e
42
âˆ’I ( P1 , P2 , M1_a,
M1_b,
0 , M1_d)
43
44 C o n s t r a i n t s
45
M1_a >= 0 ;
46
M1_a <= 0 . 5 ;
47
M1_d >= 0 ;
48
M1_aâˆ—M1_d âˆ’ M1_b^2 >= 0 ;
49
(1âˆ’M1_a) âˆ—(1âˆ’M1_d) âˆ’ (âˆ’M1_b) ^2 >= 0 ;
50 end

30

C.2
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Optimisation avec trois Ã©tats dâ€™entrÃ©e

Constants
m_amount = 2 ;
m_size = 2 ;
P1 [ m_size ] [ m_size ] = ( ( 0 . 1 0 , 0 ) ; ( 0 , 0 ) ) ;
P2 [ m_size ] [ m_size ] = ( ( 0 . 3 , 0 . 3 ) ; ( 0 , 0 . 3 ) ) ;
P3 [ m_size ] [ m_size ] = ( ( 0 , 0 ) ; ( 0 , 0 . 3 ) ) ;
// M: measurement o p e r a t o r
// P : d e n s i t y o p e r a t o r
f u n c t i o n t r (M[ m_size ] [ m_size ] , P [ m_size ] [ m_size ] )
r e t u r n P ( 1 ) ( 1 ) âˆ— M( 1 ) ( 1 ) + 2âˆ—M( 1 ) ( 2 ) âˆ—P ( 1 ) ( 2 ) + 2âˆ—M( 2 ) ( 1 ) âˆ—P ( 2 ) ( 1 ) + M( 2 ) ( 2 ) âˆ—P ( 2 ) ( 2 ) ;
end

f u n c t i o n EntropyM (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _M3[ m_size ] [ m_size ] , _P1 [
m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _P3 [ m_size ] [ m_size ] )
15
sum_m1 = t r (_M1, _P1) + t r (_M1, _P2) + t r (_M1, _P3) ;
16
sum_m2 = t r (_M2, _P1) + t r (_M2, _P2) + t r (_M2, _P3) ;
17
sum_m3 = t r (_M3, _P1) + t r (_M3, _P2) + t r (_M3, _P3) ;
18
r e t u r n âˆ’x l o g (sum_m1)âˆ’x l o g (sum_m2) âˆ’ x l o g (sum_m3) ;
19 end
20
21 f u n c t i o n EntropyP (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _M3[ m_size ] [ m_size ] , _P1 [
m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _P3 [ m_size ] [ m_size ] )
22
sum_p1 = t r (_M1, _P1) + t r (_M2, _P1) + t r (_M3, _P1) ;
23
sum_p2 = t r (_M1, _P2) + t r (_M2, _P2) + t r (_M3, _P2) ;
24
sum_p3 = t r (_M1, _P3) + t r (_M2, _P3) + t r (_M3, _P3) ;
25
r e t u r n âˆ’x l o g ( sum_p1 )âˆ’x l o g ( sum_p2 )âˆ’x l o g ( sum_p3 ) ;
26 end
27
28 f u n c t i o n EntropyMP (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _M3[ m_size ] [ m_size ] , _P1 [
m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _P3 [ m_size ] [ m_size ] )
29
r e t u r n âˆ’x l o g ( t r (_M1, _P1) )âˆ’x l o g ( t r (_M1, _P2) )âˆ’x l o g ( t r (_M1, _P3) )âˆ’x l o g ( t r (_M2, _P1) )âˆ’
x l o g ( t r (_M2, _P2) )âˆ’x l o g ( t r (_M2, _P3) )âˆ’x l o g ( t r (_M3, _P1) )âˆ’x l o g ( t r (_M3, _P2) )âˆ’x l o g ( t r (
_M3, _P3) ) ;
30 end
31
32 f u n c t i o n M u t u a l I n f o r m a t i o n (_M1[ m_size ] [ m_size ] , _M2[ m_size ] [ m_size ] , _M3[ m_size ] [ m_size
] , _P1 [ m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _P3 [ m_size ] [ m_size ] )
33
r e t u r n EntropyM (_M1, _M2, _M3, _P1 , _P2 , _P3) + ( âˆ’0.10 âˆ— l n ( 0 . 1 0 ) âˆ’0.60 âˆ— l n ( 0 . 6 0 )
âˆ’0.30 âˆ— l n ( 0 . 3 0 ) ) âˆ’ EntropyMP (_M1, _M2, _M3, _P1 , _P2 , _P3) ;
34 end
35
36 f u n c t i o n I (_P1 [ m_size ] [ m_size ] , _P2 [ m_size ] [ m_size ] , _P3 [ m_size ] [ m_size ] , _m1a , _m1b ,
_m1c , _m1d , _m2a , _m2b , _m2c , _m2d)
37
_M1 = ( ( _m1a , _m1b) ; ( _m1c , _m1d) ) ;
38
_M2 = ( ( _m2a , _m2b) ; ( _m2c , _m2d) ) ;
39
_M3 = ((1 âˆ’_m1aâˆ’_m2a , âˆ’_m1bâˆ’_m2b) ; (âˆ’_m1câˆ’_m2c , 1âˆ’_m1dâˆ’_m2d) ) ;
40
r e t u r n M u t u a l I n f o r m a t i o n (_M1, _M2, _M3, _P1 , _P2 , _P3) ;
41 end
42
43 V a r i a b l e s
44
M1_a i n [ 0 , 1 ] ,
M1_b i n [ âˆ’1 , 1 ] , M1_c i n [ âˆ’1 , 1 ] , M1_d i n [ 0 , 1 ] ;
45
M2_a i n [ 0 , 1 ] ,
M2_b i n [ âˆ’1 , 1 ] , M2_c i n [ âˆ’1 , 1 ] , M2_d i n [ 0 , 1 ] ;
46
47 M i n i m i z e
48
âˆ’I ( P1 , P2 , P3 , M1_a,
M1_b,
M1_c , M1_d, M2_a,
M2_b,
M2_c , M2_d) ;
49
50 C o n s t r a i n t s
51
M1_aâˆ—M1_d âˆ’ M1_b^2 âˆ’ M1_c^2 >= 0 ;
52
M1_a <= 1/3 ; // c o s t f u n c t i o n & c o n s t r a i n t both s y m m e t r i c ( ( M1, M2) <=> (M2, M1) )
53
M2_a <= 0 . 5 ;
54
M1_a + M1_d = 1 ;
55
M2_a + M2_d = 1 ;
56
M2_aâˆ—M2_d âˆ’ M2_b^2 âˆ’ M2_c^2 >= 0 ;
57
(1âˆ’M1_aâˆ’M2_a) >= 0 ;
58
(1âˆ’M1_aâˆ’M2_a) <= 1 ;
59
(1âˆ’M1_dâˆ’M2_d) >= 0 ;
60
(1âˆ’M1_dâˆ’M2_d) <= 1 ;
61
(1âˆ’M1_aâˆ’M2_a) âˆ—(1âˆ’M1_dâˆ’M2_d) âˆ’ (âˆ’M1_bâˆ’M2_b) ^2 âˆ’ (âˆ’M1_câˆ’M2_c) ^2 >= 0 ;
62 end

31

Bibliographie

[1] Y. C. Eldar, A. Megretski, and G. C. Verghese, â€œDesigning optimal
quantum detectors via semidefinite programming,â€ IEEE Transactions
on Information Theory, vol. 49, pp. 1007â€“1012, 2003.
[2] Y. C. Eldar and G. D. Forney, â€œOn quantum detection and the squareroot measurement,â€ IEEE Transactions on Information Theory, vol. 47,
pp. 858â€“872, 2001.
[3] E. B. Davies, â€œInformation and quantum measurement,â€ IEEE Transactions on Information Theory, vol. 24, pp. 596â€“599, 1978.
[4] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum
Information. Cambridge : Cambridge University Press, 2000.
[5] D. N. Mermin, Quantum Computer Science : An introduction. Cambridge : Cambridge University Press, 2007.
[6] E. Moore, R, â€œInterval analysis,â€ Science, vol. 158, no. 3799, pp. 365â€“
365, 1967.
[7] A. Neumaier, Interval Methods for Systems of Equations. Encyclopedia
of Mathematics and its Applications, Cambridge University Press, 1991.
[8] N. Delanoue, MÃ©thodes numÃ©riques garanties pour la classification de
fonctions et le contrÃ´le optimal. Habilitation Ã  diriger des recherches,
UniversitÃ© dâ€™Angers, 2018.
[9] P. Engelstein, N. Delanoue, and F. Chapeau-Blondeau, â€œConception de
dÃ©tecteurs quantiques optimaux via le calcul par intervalles,â€ JournÃ©e
"Traitement du signal et applications quantiques" du GdR CNRS ISIS
(Information Signal Image viSion). PrÃ©vue le 22 juin 2021 puis reportÃ©e.
[10] C. E. Shannon, â€œA mathematical theory of communication,â€ The Bell
System Technical Journal, vol. 27, no. 3, pp. 379â€“423, 1948.
[11] T. M. Cover and J. A. Thomas, Elements of Information Theory. New
York : Wiley, 1991.
[12] Y. Shi, â€œBoth toffoli and controlled-not need little help to do universal
quantum computing,â€ Quantum Info. Comput., vol. 3, p. 84â€“92, Jan.
2003.
32

[13] D. Deutsch and R. Jozsa, â€œRapid solution of problems by quantum
computation,â€ Proceedings of the Royal Society of London A, vol. 439,
pp. 553â€“558, 1992.
[14] L. K. Grover, A Fast Quantum Mechanical Algorithm for Database
Search, p. 212â€“219. STOC â€™96, New York, NY, USA : Association for
Computing Machinery, 1996.
[15] P. W. Shor, â€œPolynomial-time algorithms for prime factorization and
discrete logarithms on a quantum computer,â€ SIAM Journal on Computing, vol. 26, pp. 1484â€“1509, 1997.
[16] A. Younes and J. Miller, â€œAutomated method for building cnot based
quantum circuits for boolean functions,â€ In Proceeding of ICENCO2004,
pp. 562â€“565, 2003.

33

RÃ©sumÃ© â€” Ce mÃ©moire sâ€™intÃ©resse Ã  la question de la dÃ©tection optimale
quantique. On dispose dâ€™un ensemble dâ€™Ã©tats quantiques que lâ€™on veut dÃ©tecter le plus efficacement possible par le biais dâ€™un ensemble dâ€™opÃ©rateurs de
mesure. Plusieurs critÃ¨res de performance existent tels que lâ€™erreur de mesure
ou lâ€™erreur quadratique. On Ã©tudie ici le critÃ¨re de lâ€™information mutuelle, en
y appliquant les techniques de calcul par intervalles pour obtenir un rÃ©sultat
global garanti.
Mots clÃ©s : Informatique quantique, dÃ©tection optimale, opÃ©rateur de
mesure, information mutuelle, calcul par intervalles.

Abstract â€” This report presents the problem of optimal quantum detection. We are provided with a set of quantum states with prior probabilities,
and we want to build the set of quantum measurement operators to have
an optimal measure. Multiple performance criterion exist, such as the probability of error of detection, or the square root error. We use the mutual
information, using interval analysis to provide a global guaranteed solution.
Keywords : Quantum computing, quantum optimal detection, measurement operator, mutual information, interval analysis.

Polytech Angers
62, avenue Notre Dame du Lac
49000 Angers

